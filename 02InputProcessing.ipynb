{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK5w54E7vu+bJpypBOxlo5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformer-based embedding model for text typically involves:\n",
        "\n",
        "1) Embedding + Positional Encoding to convert tokens into a learnable representation with positional information.\n",
        "\n",
        "\n",
        "2) A stack of self-attention + feed-forward layers (the transformer encoder) to build context-aware embeddings.\n",
        "\n",
        "3) A pooling mechanism (often [CLS] or mean pooling) to derive a single vector per sentence or document if needed.\n",
        "\n",
        "4) Pretraining on large-scale corpora (often with masked language modeling),\n",
        "\n",
        "5) Fine-tuning on specific tasks.\n",
        "\n",
        "These steps yield high-quality text embeddings that capture semantic relationships and context making transformers the top choice for modern NLP tasks.\n"
      ],
      "metadata": {
        "id": "POXi91AxDt2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9XH0LtA_PF8",
        "outputId": "af238c4a-56c6-42e5-93aa-3c248186f30d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
              " [0.7, 0.8, 0.9, 0.1, 0.2, 0.3],\n",
              " [0.4, 0.3, 0.2, 0.1, 0.9, 0.8]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Full Toy Example Python\n",
        "\n",
        "# Example small vocabulary (toy example)\n",
        "vocab = {\n",
        "    \"I\": 0,\n",
        "    \"love\": 1,\n",
        "    \"cats\": 2,\n",
        "    \"dogs\": 3\n",
        "}\n",
        "\n",
        "# Let's define an embedding dimension:\n",
        "EMBED_DIM = 6\n",
        "\n",
        "# Manually define an embedding table for demonstration\n",
        "# This is a list of 'vocab_size' lists, each inner list has length EMBED_DIM.\n",
        "# In a real scenario, these would be learned parameters.\n",
        "EMBEDDING_TABLE = [\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],  # embedding for vocab ID 0\n",
        "    [0.7, 0.8, 0.9, 0.1, 0.2, 0.3],  # embedding for vocab ID 1\n",
        "    [0.4, 0.3, 0.2, 0.1, 0.9, 0.8],  # embedding for vocab ID 2\n",
        "    [0.1, 0.5, 0.4, 0.4, 0.4, 0.2]   # embedding for vocab ID 3\n",
        "]\n",
        "\n",
        "def embed_tokens(token_ids):\n",
        "    \"\"\"\n",
        "    Given a list of token IDs, return a list of embedding vectors.\n",
        "    Each vector is length EMBED_DIM.\n",
        "    \"\"\"\n",
        "    embedded_sequence = []\n",
        "    for tid in token_ids:\n",
        "        embedded_sequence.append(EMBEDDING_TABLE[tid])  # simple lookup\n",
        "    return embedded_sequence  # shape: (sequence_length x EMBED_DIM)\n",
        "\n",
        "# Example sentence\n",
        "sentence_tokens = [\"I\", \"love\", \"cats\"]\n",
        "\n",
        "# 1. Convert tokens to IDs\n",
        "token_ids = [vocab[tok] for tok in sentence_tokens]  # e.g. [0, 1, 2]\n",
        "\n",
        "# 2. Embed these tokens\n",
        "embedded_seq = embed_tokens(token_ids)\n",
        "# embedded_seq might look like:\n",
        "# [\n",
        "#   [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "#   [0.7, 0.8, 0.9, 0.1, 0.2, 0.3],\n",
        "#   [0.4, 0.3, 0.2, 0.1, 0.9, 0.8]\n",
        "# ]\n",
        "embedded_seq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# We loop over positions 0 to max_seq_len - 1.\n",
        "# For each dimension pair (2i, 2i+1), we compute sine and cosine.\n",
        "# Note that in practice,\n",
        "# many frameworks vectorize this calculation for speed,\n",
        "# but the logic remains the same.\n",
        "\n",
        "\n",
        "def create_positional_encodings(max_seq_len, embedding_dim):\n",
        "    \"\"\"\n",
        "    Generates positional encodings for positions [0..max_seq_len-1].\n",
        "    Returns a list of size `max_seq_len`, where each item is\n",
        "    a list of length `embedding_dim`.\n",
        "    \"\"\"\n",
        "\n",
        "    position_encodings = []\n",
        "    for pos in range(max_seq_len):\n",
        "        encoding = [0.0]*embedding_dim\n",
        "        for i in range(0,embedding_dim,2):\n",
        "            angle_rate = (1.0/math.pow(10000, (2*i)/embedding_dim))\n",
        "            encoding[i] = math.sin(pos*angle_rate)\n",
        "            if i+1 < embedding_dim:\n",
        "                encoding[i+1] = math.cos(pos*angle_rate)\n",
        "        position_encodings.append(encoding)\n",
        "\n",
        "    return position_encodings\n",
        "\n",
        "# 3. Create positional encodings up to max_seq_len = 10 (arbitrary)\n",
        "pos_enc = create_positional_encodings(10, EMBED_DIM)\n",
        "# pos_enc is a list with 100 items, each item is a 6-element list\n",
        "\n",
        "for idx,p in enumerate(pos_enc):\n",
        "    print(idx,list(f\"{x:.2f}\" for x in p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYTYquA-_S7X",
        "outputId": "65ddc515-72a5-4e6e-a220-a65659f63631"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['0.00', '1.00', '0.00', '1.00', '0.00', '1.00']\n",
            "1 ['0.84', '0.54', '0.00', '1.00', '0.00', '1.00']\n",
            "2 ['0.91', '-0.42', '0.00', '1.00', '0.00', '1.00']\n",
            "3 ['0.14', '-0.99', '0.01', '1.00', '0.00', '1.00']\n",
            "4 ['-0.76', '-0.65', '0.01', '1.00', '0.00', '1.00']\n",
            "5 ['-0.96', '0.28', '0.01', '1.00', '0.00', '1.00']\n",
            "6 ['-0.28', '0.96', '0.01', '1.00', '0.00', '1.00']\n",
            "7 ['0.66', '0.75', '0.02', '1.00', '0.00', '1.00']\n",
            "8 ['0.99', '-0.15', '0.02', '1.00', '0.00', '1.00']\n",
            "9 ['0.41', '-0.91', '0.02', '1.00', '0.00', '1.00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining Token Embeddings with Positional Encodings\n",
        "#To get the final input to the transformer’s encoder (for a single sequence), we sum:\n",
        "\n",
        "# 𝐸token +𝐸 positional\n",
        "\n",
        "#where each is a list (or array) of size EMBED_DIM. Here’s a simple example:\n",
        "\n",
        "def add_positional_encodings(embedded_sequence, positional_encodings):\n",
        "    \"\"\"\n",
        "    Sums the token embeddings with the positional encodings.\n",
        "    Both are lists of lists of length `embedding_dim`.\n",
        "\n",
        "    embedded_sequence: shape (seq_len x EMBED_DIM)\n",
        "    positional_encodings: shape (max_seq_len x EMBED_DIM)\n",
        "    \"\"\"\n",
        "    assert len(embedded_sequence[0]) == len(positional_encodings[0])\n",
        "\n",
        "    seq_len = len(embedded_sequence)\n",
        "    encoded_sequence = list()\n",
        "\n",
        "    for p in range(seq_len):\n",
        "        token_vec = embedded_sequence[p]\n",
        "        pos_vec = positional_encodings[p]\n",
        "\n",
        "        combined_vec = [token_vec[i] + pos_vec[i] for i in range(len(token_vec))]\n",
        "        encoded_sequence.append(combined_vec)\n",
        "\n",
        "    assert len(encoded_sequence) == len(embedded_sequence)\n",
        "\n",
        "    return encoded_sequence\n",
        "\n"
      ],
      "metadata": {
        "id": "1WNnyPmXHsXG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Combine the embeddings with the corresponding positional vectors\n",
        "final_encoded_seq = add_positional_encodings(embedded_seq, pos_enc)\n",
        "# final_encoded_seq is also shape (3 x 6), representing the input to the first Transformer layer.\n",
        "\n",
        "\n",
        "print(\"Embedded sequence (no position):\")\n",
        "for idx, row in enumerate(embedded_seq):\n",
        "    print(idx, list(f\"{x:.2f}\" for x in row))\n",
        "\n",
        "print(\"\\nPositional encodings (first 3 positions):\")\n",
        "for idx in range(len(sentence_tokens)):\n",
        "    p = pos_enc[idx]\n",
        "    print(idx, list(f\"{x:.2f}\" for x in p))\n",
        "\n",
        "print(\"\\nCombined (token + positional):\")\n",
        "for idx, row in enumerate(final_encoded_seq):\n",
        "    print(idx, list(f\"{x:.2f}\" for x in row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRGBRP-qLX5V",
        "outputId": "a783aaeb-e0f6-4daf-b382-9a05bd4d9269"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded sequence (no position):\n",
            "0 ['0.10', '0.20', '0.30', '0.40', '0.50', '0.60']\n",
            "1 ['0.70', '0.80', '0.90', '0.10', '0.20', '0.30']\n",
            "2 ['0.40', '0.30', '0.20', '0.10', '0.90', '0.80']\n",
            "\n",
            "Positional encodings (first 3 positions):\n",
            "0 ['0.00', '1.00', '0.00', '1.00', '0.00', '1.00']\n",
            "1 ['0.84', '0.54', '0.00', '1.00', '0.00', '1.00']\n",
            "2 ['0.91', '-0.42', '0.00', '1.00', '0.00', '1.00']\n",
            "\n",
            "Combined (token + positional):\n",
            "0 ['0.10', '1.20', '0.30', '1.40', '0.50', '1.60']\n",
            "1 ['1.54', '1.34', '0.90', '1.10', '0.20', '1.30']\n",
            "2 ['1.31', '-0.12', '0.20', '1.10', '0.90', '1.80']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(vocab_size, embedding_dim, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return rng.normal(loc=0.0, scale=0.1, size=(vocab_size, embedding_dim))\n",
        "\n",
        "def embed_tokens(token_ids, embedding_matrix):\n",
        "    batch_size, seq_len = token_ids.shape\n",
        "    embed_dim = embedding_matrix.shape[1]\n",
        "    output = np.zeros((batch_size, seq_len, embed_dim), dtype=np.float32)\n",
        "    for b in range(batch_size):\n",
        "        for p in range(seq_len):\n",
        "            tid = token_ids[b, p]\n",
        "            output[b, p, :] = embedding_matrix[tid]\n",
        "    return output\n",
        "\n",
        "def create_positional_encodings(max_len, d_model):\n",
        "    pos_enc = np.zeros((max_len, d_model), dtype=np.float32)\n",
        "    for pos in range(max_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            angle_rate = 1.0 / (10000.0 ** ((2.0 * i) / d_model))\n",
        "            pos_enc[pos, i] = math.sin(pos * angle_rate)\n",
        "            if i + 1 < d_model:\n",
        "                pos_enc[pos, i + 1] = math.cos(pos * angle_rate)\n",
        "    return pos_enc\n",
        "\n",
        "def add_positional_encoding(embedded_tokens, pos_enc):\n",
        "    batch_size, seq_len, embed_dim = embedded_tokens.shape\n",
        "    for p in range(seq_len):\n",
        "        embedded_tokens[:, p, :] += pos_enc[p]\n",
        "    return embedded_tokens\n",
        "\n",
        "# ---- Example usage ----\n",
        "\n",
        "# 1) Create random embedding weights for a small vocab\n",
        "vocab_size = 4\n",
        "embed_dim = 6\n",
        "embedding_matrix = create_embedding_matrix(vocab_size, embed_dim)\n",
        "\n",
        "# 2) Prepare a mini-batch of token IDs: shape (batch_size=2, seq_len=3)\n",
        "#    Example:\n",
        "#    Sequence 1 -> [0, 1, 2]\n",
        "input_ids = np.array([\n",
        "    [0, 1, 2],\n",
        "], dtype=np.int64)  # shape: (2, 3)\n",
        "\n",
        "# 3) Embed the tokens\n",
        "embedded_tokens = embed_tokens(input_ids, embedding_matrix)\n",
        "# shape: (2, 3, 6)\n",
        "\n",
        "# 4) Create positional encodings up to max_len=10 (just to be safe)\n",
        "pos_enc = create_positional_encodings(max_len=10, d_model=embed_dim)\n",
        "# shape: (10, 6)\n",
        "\n",
        "# 5) Add positional encodings to the token embeddings\n",
        "embedded_with_pos = add_positional_encoding(embedded_tokens, pos_enc)\n",
        "# shape remains (1, 3, 6)\n",
        "\n",
        "print(\"Final shape:\", embedded_with_pos.shape)\n",
        "print(embedded_with_pos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b58GSILYf1",
        "outputId": "10b54d37-eed4-41bb-d35d-1a7e7c91e0be"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final shape: (1, 3, 6)\n",
            "[[[ 3.0471709e-02  8.9600158e-01  7.5045116e-02  1.0940565e+00\n",
            "   -1.9510353e-01  8.6978203e-01]\n",
            "  [ 8.5425502e-01  5.0867802e-01  4.7431723e-04  9.1469330e-01\n",
            "    8.7944441e-02  1.0777792e+00]\n",
            "  [ 9.1590047e-01 -3.0342272e-01  5.1059790e-02  9.1406143e-01\n",
            "    3.6884360e-02  9.0411174e-01]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsdSHeXON3Qq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}