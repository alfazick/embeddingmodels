{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaIMW0FcBoDgLNKOqXqwUg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Architecture of Embedding Models.**\n",
        "\n",
        "which are neural networks designed to convert text, images, or\n",
        "other data into dense vector representations.\n",
        "Key components of embedding model architecture:\n",
        "\n",
        "**Stage#1 The input layer processes raw data** (like text tokens or image pixels) and typically includes preprocessing steps:\n",
        "\n",
        "For text: Tokenization breaks input into subword units\n",
        "For images: Convolutional layers extract visual features\n",
        "Positional encodings may be added to maintain sequence order\n",
        "\n",
        "**Stage#2 The encoder layers then transform this input into increasingly abstract representations**:\n",
        "\n",
        "Multiple transformer blocks or deep neural network layers\n",
        "Each layer typically consists of:\n",
        "\n",
        "Self-attention mechanisms to capture relationships between elements\n",
        "Feed-forward neural networks to process these relationships\n",
        "Layer normalization and residual connections\n",
        "\n",
        "\n",
        "Stage #3 The pooling layer combines the encoded representations:\n",
        "**bold text**\n",
        "Mean pooling takes the average across all encoded elements\n",
        "Max pooling selects the strongest features\n",
        "[CLS] token pooling (common in BERT-style models) uses a special token to aggregate sequence information\n",
        "\n",
        "**Stage #4 The final embedding layer projects the pooled representation into the desired embedding space**:\n",
        "\n",
        "Linear transformation to achieve target embedding dimension\n",
        "Normalization may be applied (L2 norm, etc.)\n",
        "\n",
        "The resulting embedding vector captures semantic meaning in a fixed-dimensional space\n",
        "\n",
        "Training objectives shape how these embeddings capture relationships:\n",
        "\n",
        "Contrastive learning pulls similar items together and pushes dissimilar items apart\n",
        "Masked language modeling predicts missing tokens\n",
        "Next sentence prediction determines if sequences are related\n",
        "Classification tasks supervise embeddings to capture relevant features\n",
        "\n",
        "The specific architecture choices depend on the use case:\n",
        "\n",
        "Text embeddings may prioritize capturing linguistic relationships\n",
        "Image embeddings focus on visual feature hierarchies\n",
        "Cross-modal embeddings align representations across different types of data"
      ],
      "metadata": {
        "id": "xSCqR2iJlpfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vpGbeMghkE9B"
      },
      "outputs": [],
      "source": [
        "#1 What is input processing?\n",
        "\n",
        "# a) Tokenization:Converting raw text into token IDs\n",
        "# b) Token Embeddings:Converting token IDs into vectors\n",
        "# c) Position Embeddings:Adding positional information\n",
        "# d) Layer Normalization:Stabilizing the representation\n",
        "# e) Droput: Applyuing regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple embedding model\n",
        "import numpy as np\n",
        "\n",
        "class SimpleEmbeddingModel:\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding_matrix = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
        "\n",
        "    def embed(self, input_ids):\n",
        "        \"\"\"Convert token IDs to embeddings\"\"\"\n",
        "        return self.embedding_matrix[input_ids]"
      ],
      "metadata": {
        "id": "DhOAkyMkoQxM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SimpleEmbeddingModel(5,3).embedding_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT0kKU4zsJt4",
        "outputId": "7152af4f-e68d-483b-bf00-f3559897cfbd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0749917 , -0.05158836, -0.06308906],\n",
              "       [ 0.04143047, -0.06643029,  0.02618513],\n",
              "       [-0.08275793, -0.09106689,  0.0705244 ],\n",
              "       [ 0.08773272, -0.00780366, -0.05142994],\n",
              "       [ 0.011674  ,  0.01717707,  0.0930771 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tiny vocabulary\n",
        "vocab = {\n",
        "    \"hello\": 0,\n",
        "    \"world\": 1,\n",
        "    \"hi\": 2,\n",
        "    \"earth\": 3\n",
        "}\n",
        "\n",
        "model = SimpleEmbeddingModel(vocab_size=len(vocab), embedding_dim=3)\n",
        "sentense = \"hello world\"\n",
        "input_ids = [vocab[word] for word in sentense.split()]\n",
        "embeddings = model.embed(input_ids)\n",
        "print(\"Embeddings shape:\", embeddings.shape)  # Should be (2, 3)\n",
        "print(\"Embeddings:\", embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdDkN77poU_G",
        "outputId": "73b879a8-bd81-4973-884b-f618b777f125"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (2, 3)\n",
            "Embeddings: [[-0.02467514 -0.08158735 -0.07707473]\n",
            " [ 0.0938968  -0.07724188 -0.04269936]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "d_model = 4\n",
        "\n",
        "# For one position, we get a vector of 4 numbers\n",
        "# Calculate frequencies\n",
        "freq1 = 1/np.power(10000, 0/d_model)    # = 1\n",
        "freq2 = 1/np.power(10000, 2/d_model)    # smaller frequency\n",
        "\n",
        "print(\"Frequencies:\")\n",
        "print(f\"freq1 = {freq1}\")\n",
        "print(f\"freq2 = {freq2}\")\n",
        "\n",
        "# Position 0 gets:\n",
        "pos0 = [\n",
        "    np.sin(0 * freq1),   # index 0\n",
        "    np.cos(0 * freq1),   # index 1\n",
        "    np.sin(0 * freq2),   # index 2\n",
        "    np.cos(0 * freq2)    # index 3\n",
        "]\n",
        "\n",
        "# Position 1 gets:\n",
        "pos1 = [\n",
        "    np.sin(1 * freq1),   # index 0\n",
        "    np.cos(1 * freq1),   # index 1\n",
        "    np.sin(1 * freq2),   # index 2\n",
        "    np.cos(1 * freq2)    # index 3\n",
        "]\n",
        "\n",
        "print(\"\\nPosition 0 vector:\", pos0)\n",
        "print(\"Position 1 vector:\", pos1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heJHXg0Jxr_u",
        "outputId": "df4561fe-8f5b-490a-d3af-e3a5544278df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequencies:\n",
            "freq1 = 1.0\n",
            "freq2 = 0.01\n",
            "\n",
            "Position 0 vector: [0.0, 1.0, 0.0, 1.0]\n",
            "Position 1 vector: [0.8414709848078965, 0.5403023058681398, 0.009999833334166664, 0.9999500004166653]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each position, you create a vector where:\n",
        "\n",
        "Elements alternate between sin and cos\n",
        "Each sin/cos PAIR shares the same frequency\n",
        "Different pairs use different frequencies\n",
        "\n"
      ],
      "metadata": {
        "id": "fwTarBgB1uMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# positional encoding\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_position_encodings(max_seq_len:int, embedding_dim:int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        max_seq_length: how many positions we need to encode (e.g., 512)\n",
        "        embedding_dim: size of each position's encoding vector (e.g., 4)\n",
        "                      must be even because we use sin/cos pairs\n",
        "    Returns:\n",
        "        Array of shape (max_seq_length, embedding_dim)\n",
        "        Each row is a position's encoding vector\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    position_encodings = np.zeros((max_seq_len, embedding_dim))\n",
        "\n",
        "\n",
        "    for pos in range(max_seq_len):\n",
        "        for dim in range(0,embedding_dim,2):\n",
        "\n",
        "        # original paper \"attention is all you need\" 1000**(-dim/embedding_dim)\n",
        "            freq = np.exp(dim * -np.log(10000.0) / embedding_dim)\n",
        "\n",
        "        # because pow(a,b) = exp(b*ln(a)) for all a > 0\n",
        "\n",
        "\n",
        "            # First in pair: sine\n",
        "            position_encodings[pos, dim] = np.sin(pos * freq)\n",
        "\n",
        "            # Second in pair: cosine(if it exists)\n",
        "\n",
        "            if dim+1< embedding_dim:\n",
        "                position_encodings[pos, dim+1] = np.cos(pos * freq)\n",
        "\n",
        "    return position_encodings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EsItlPsasD6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}