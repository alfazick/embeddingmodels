{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOOL4D6/tET0x+2vJhCGM2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6UCB7np7qg7",
        "outputId": "2b59a13d-01e4-4566-87fe-819a2af71d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello tokens\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization is a task of breaking down the text into smaller pieces\n",
        "# Embeddings. Turn tokens into numeric representations\n",
        "# Contextualized Embeddings. Where each token considers other tokens in text\n",
        "\n",
        "# Subword tokens are used for represiinting words in LLM\n",
        "# Example: doing => do, ###ing"
      ],
      "metadata": {
        "id": "DU1bxRx07w0h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZAGEUjz93gb",
        "outputId": "64e7a732-d900-4e17-9698-5ef366b2a6c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing Text\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "sentence = \"what does capital means?\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zpR8vwd3966v"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer(sentence).input_ids"
      ],
      "metadata": {
        "id": "x4ktexbs-mGy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--htgepM-9QL",
        "outputId": "88df78a9-bcb9-424a-bc32-8b389388c488"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 1184, 1674, 2364, 2086, 136, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in token_ids:\n",
        "    print(token_id,tokenizer.decode(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyqvWbjM--fe",
        "outputId": "7d15d160-481b-46fb-b015-ab9428e0c038"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 [CLS]\n",
            "1184 what\n",
            "1674 does\n",
            "2364 capital\n",
            "2086 means\n",
            "136 ?\n",
            "102 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2 = \"what does CaPiTal MeaN$?\"\n",
        "for token_id in tokenizer(sentence2)[\"input_ids\"]:\n",
        "    print(token_id,tokenizer.decode(token_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AlkE1pK_PJs",
        "outputId": "15c7d732-5de2-42bb-de64-4bbda4891b94"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 [CLS]\n",
            "1184 what\n",
            "1674 does\n",
            "140 C\n",
            "1161 ##a\n",
            "2101 ##P\n",
            "1182 ##i\n",
            "1942 ##T\n",
            "1348 ##al\n",
            "2508 Me\n",
            "1161 ##a\n",
            "2249 ##N\n",
            "109 $\n",
            "136 ?\n",
            "102 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A list of colors in RGB for representing the tokens\n",
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]"
      ],
      "metadata": {
        "id": "8ufE7sh4_tw-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tokens(sentence: str, tokenizer_name: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, token in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(token) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n"
      ],
      "metadata": {
        "id": "KNthAM_VAr1r"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(sentence, \"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd261uHSBSeo",
        "outputId": "f5a87037-2e2c-42ef-d765-23acd27aeab9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 28996\n",
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mdoes\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84mmeans\u001b[0m \u001b[0;30;48;2;255;217;47m?\u001b[0m \u001b[0;30;48;2;102;194;165m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(sentence2, \"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ3WEo2iBVwY",
        "outputId": "da4d071e-7714-4738-b210-8949bdaaac9a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 28996\n",
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mdoes\u001b[0m \u001b[0;30;48;2;231;138;195mC\u001b[0m \u001b[0;30;48;2;166;216;84m##a\u001b[0m \u001b[0;30;48;2;255;217;47m##P\u001b[0m \u001b[0;30;48;2;102;194;165m##i\u001b[0m \u001b[0;30;48;2;252;141;98m##T\u001b[0m \u001b[0;30;48;2;141;160;203m##al\u001b[0m \u001b[0;30;48;2;231;138;195mMe\u001b[0m \u001b[0;30;48;2;166;216;84m##a\u001b[0m \u001b[0;30;48;2;255;217;47m##N\u001b[0m \u001b[0;30;48;2;102;194;165m$\u001b[0m \u001b[0;30;48;2;252;141;98m?\u001b[0m \u001b[0;30;48;2;141;160;203m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(sentence2, \"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YvxlzWFBZZa",
        "outputId": "e0f573ac-8d44-4f6d-c546-5e71189c9dd9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 30522\n",
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mdoes\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84mmean\u001b[0m \u001b[0;30;48;2;255;217;47m$\u001b[0m \u001b[0;30;48;2;102;194;165m?\u001b[0m \u001b[0;30;48;2;252;141;98m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(sentence, \"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4JGiHmKBiqH",
        "outputId": "4bec244c-d7f4-4415-86aa-78698dde534c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 30522\n",
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mdoes\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84mmeans\u001b[0m \u001b[0;30;48;2;255;217;47m?\u001b[0m \u001b[0;30;48;2;102;194;165m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "side_by_side = [sentence,sentence2]\n",
        "\n",
        "for sentence in side_by_side:\n",
        "    show_tokens(sentence, \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tFVNaxwBqEv",
        "outputId": "c709f31b-2d8b-49aa-9181-40027294151c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 128256\n",
            "\u001b[0;30;48;2;102;194;165m<｜begin▁of▁sentence｜>\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203m does\u001b[0m \u001b[0;30;48;2;231;138;195m capital\u001b[0m \u001b[0;30;48;2;166;216;84m means\u001b[0m \u001b[0;30;48;2;255;217;47m?\u001b[0m Vocab length: 128256\n",
            "\u001b[0;30;48;2;102;194;165m<｜begin▁of▁sentence｜>\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203m does\u001b[0m \u001b[0;30;48;2;231;138;195m Ca\u001b[0m \u001b[0;30;48;2;166;216;84mPi\u001b[0m \u001b[0;30;48;2;255;217;47mT\u001b[0m \u001b[0;30;48;2;102;194;165mal\u001b[0m \u001b[0;30;48;2;252;141;98m Me\u001b[0m \u001b[0;30;48;2;141;160;203maN\u001b[0m \u001b[0;30;48;2;231;138;195m$\u001b[0m \u001b[0;30;48;2;166;216;84m?\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in side_by_side:\n",
        "    show_tokens(sentence, \"Qwen/Qwen2-VL-7B-Instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC1R80nUB_iA",
        "outputId": "c451d8ed-7551-4089-ab35-a6eea7460ccc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 151657\n",
            "\u001b[0;30;48;2;102;194;165mwhat\u001b[0m \u001b[0;30;48;2;252;141;98m does\u001b[0m \u001b[0;30;48;2;141;160;203m capital\u001b[0m \u001b[0;30;48;2;231;138;195m means\u001b[0m \u001b[0;30;48;2;166;216;84m?\u001b[0m Vocab length: 151657\n",
            "\u001b[0;30;48;2;102;194;165mwhat\u001b[0m \u001b[0;30;48;2;252;141;98m does\u001b[0m \u001b[0;30;48;2;141;160;203m Ca\u001b[0m \u001b[0;30;48;2;231;138;195mPi\u001b[0m \u001b[0;30;48;2;166;216;84mT\u001b[0m \u001b[0;30;48;2;255;217;47mal\u001b[0m \u001b[0;30;48;2;102;194;165m Me\u001b[0m \u001b[0;30;48;2;252;141;98maN\u001b[0m \u001b[0;30;48;2;141;160;203m$\u001b[0m \u001b[0;30;48;2;231;138;195m?\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfFSYmt0CxUA"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}